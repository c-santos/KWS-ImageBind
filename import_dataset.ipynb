{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install pytorch_lightning\n",
        "%pip install torchmetrics"
      ],
      "metadata": {
        "id": "FCdV0jFsNW0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio, torchvision\n",
        "\n",
        "import matplotlib as plt\n",
        "import librosa\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "from torchmetrics.functional import accuracy\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS"
      ],
      "metadata": {
        "id": "18iynPlJKJHm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "gqe2TA1dNjoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Silence and Unknown Dataset\n",
        "\n",
        "We create a custom `silence` dataset. The dataset randomly samples background audio supplied in the KWS dataset. These files are under the `_background_noise_` folder.\n",
        "\n",
        "We also create `unknown` dataset uses random audio samples from the train set but labelled as `unknown`. \n",
        "\n",
        "The creation of these 2 datasets is described in the [KWS paper](https://arxiv.org/pdf/1804.03209.pdf) and implemented below.\n",
        "\n",
        "We limit the number of samples to about the size of train dataset divided by 35 (the number of distinct words in the KWS dataset)."
      ],
      "metadata": {
        "id": "5xNBPPZlMuAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SilenceDataset(SPEECHCOMMANDS):\n",
        "    def __init__(self, root):\n",
        "        super(SilenceDataset, self).__init__(root, subset='training')\n",
        "        self.len = len(self._walker) // 35\n",
        "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
        "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = np.random.randint(0, len(self.paths))\n",
        "        filepath = self.paths[index]\n",
        "        waveform, sample_rate = torchaudio.load(filepath)\n",
        "        return waveform, sample_rate, \"silence\", 0, 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "class UnknownDataset(SPEECHCOMMANDS):\n",
        "    def __init__(self, root):\n",
        "        super(UnknownDataset, self).__init__(root, subset='training')\n",
        "        self.len = len(self._walker) // 35\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = np.random.randint(0, len(self._walker))\n",
        "        fileid = self._walker[index]\n",
        "        waveform, sample_rate, _, speaker_id, utterance_number = torchaudio.datasets.speechcommands.load_speechcommands_item(fileid, self._path)\n",
        "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "fDKQkP50LJ1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The PyTorch Lightning Data Module for KWS\n",
        "\n",
        "`KWSDataModule` cleanly separates the data handling from the model. The data module handles the datasets and dataloaders.\n",
        "\n",
        "We use `torchaudio` `SPEECHCOMMANDS` dataset to load the training, testing and validation sets. \n",
        "\n",
        "A custom `collate_fn` is used to handle the different lengths of the audio samples. The function also converts the wav files into mel spectrograms for the ResNet18 model input layer. A mel spectrogram is a log-mel spectrogram. It is an image that shows the power spectrum of the audio signal in dB. Basically, we convert audio to image. Then, we can use an image classifier like ResNet18."
      ],
      "metadata": {
        "id": "IpnIOqmmNtFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KWSDataModule(LightningDataModule):\n",
        "    def __init__(self, path, batch_size=128, num_workers=0, n_fft=512, \n",
        "                 n_mels=128, win_length=None, hop_length=256, class_dict={}, \n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.n_fft = n_fft\n",
        "        self.n_mels = n_mels\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.class_dict = class_dict\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
        "                                                                download=True,\n",
        "                                                                subset='training')\n",
        "\n",
        "        silence_dataset = SilenceDataset(self.path)\n",
        "        unknown_dataset = UnknownDataset(self.path)\n",
        "        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
        "                                                                \n",
        "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
        "                                                              download=True,\n",
        "                                                              subset='validation')\n",
        "        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
        "                                                               download=True,\n",
        "                                                               subset='testing')                                                    \n",
        "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
        "        self.sample_rate = sample_rate\n",
        "        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
        "                                                              n_fft=self.n_fft,\n",
        "                                                              win_length=self.win_length,\n",
        "                                                              hop_length=self.hop_length,\n",
        "                                                              n_mels=self.n_mels,\n",
        "                                                              power=2.0)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.prepare_data()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collate_fn\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collate_fn\n",
        "        )\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collate_fn\n",
        "        )\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        mels = []\n",
        "        labels = []\n",
        "        wavs = []\n",
        "        for sample in batch:\n",
        "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
        "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
        "            if waveform.shape[-1] < sample_rate:\n",
        "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
        "            elif waveform.shape[-1] > sample_rate:\n",
        "                waveform = waveform[:,:sample_rate]\n",
        "\n",
        "            # mel from power to db\n",
        "            mels.append(ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max)))\n",
        "            labels.append(torch.tensor(self.class_dict[label]))\n",
        "            wavs.append(waveform)\n",
        "\n",
        "        mels = torch.stack(mels)\n",
        "        labels = torch.stack(labels)\n",
        "        wavs = torch.stack(wavs)\n",
        "   \n",
        "        return mels, labels, wavs"
      ],
      "metadata": {
        "id": "HAsO_JBYNvpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Program Arguments\n",
        "\n",
        "The default configuration is shown below. \n",
        "\n",
        "We also define `plot_waveform` for plotting the waveform of the audio samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "bIogZCZvOiHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # model training hyperparameters\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
        "                        help='number of epochs to train (default: 30)')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='learning rate (default: 0.001)')\n",
        "\n",
        "    # where dataset will be stored\n",
        "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
        "\n",
        "    # 35 keywords + silence + unknown\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
        "   \n",
        "    # mel spectrogram parameters\n",
        "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
        "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
        "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
        "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
        "\n",
        "    # 16-bit fp model to reduce the size\n",
        "    parser.add_argument(\"--precision\", default=16)\n",
        "    parser.add_argument(\"--accelerator\", default='gpu')\n",
        "    parser.add_argument(\"--devices\", default=1)\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=48)\n",
        "\n",
        "    parser.add_argument(\"--no-wandb\", default=False, action='store_true')\n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "    return args\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "        if num_channels > 1:\n",
        "            axes[c].set_ylabel(f'Channel {c+1}')\n",
        "        if xlim:\n",
        "            axes[c].set_xlim(xlim)\n",
        "        if ylim:\n",
        "            axes[c].set_ylim(ylim)\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)"
      ],
      "metadata": {
        "id": "GPSInK02Oiq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and importing the Speech Commands dataset"
      ],
      "metadata": {
        "id": "z7LQ_y4rOynq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = get_args()\n",
        "    CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
        "               'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
        "               'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
        "               'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
        "    \n",
        "    # make a dictionary from CLASSES to integers\n",
        "    CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
        "\n",
        "    if not os.path.exists(args.path):\n",
        "        os.makedirs(args.path, exist_ok=True)\n",
        "\n",
        "    datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
        "                               path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
        "                               win_length=args.win_length, hop_length=args.hop_length,\n",
        "                               class_dict=CLASS_TO_IDX)\n",
        "    datamodule.setup()\n",
        "\n",
        "    # Setup Image Bind here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b5Cel2CQO4LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\n",
        "model = model.load_from_checkpoint(os.path.join(\n",
        "    args.path, \"checkpoints\", \"resnet18-kws-best-acc.ckpt\"))\n",
        "model.eval()\n",
        "script = model.to_torchscript()\n",
        "\n",
        "# save for use in production environment\n",
        "model_path = os.path.join(args.path, \"checkpoints\",\n",
        "                          \"resnet18-kws-best-acc.pt\")\n",
        "torch.jit.save(script, model_path)\n",
        "\n",
        "# list wav files given a folder\n",
        "label = CLASSES[2:]\n",
        "label = np.random.choice(label)\n",
        "path = os.path.join(args.path, \"SpeechCommands/speech_commands_v0.02/\")\n",
        "path = os.path.join(path, label)\n",
        "wav_files = [os.path.join(path, f)\n",
        "             for f in os.listdir(path) if f.endswith('.wav')]\n",
        "# select random wav file\n",
        "wav_file = np.random.choice(wav_files)\n",
        "waveform, sample_rate = torchaudio.load(wav_file)\n",
        "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
        "                                                 n_fft=args.n_fft,\n",
        "                                                 win_length=args.win_length,\n",
        "                                                 hop_length=args.hop_length,\n",
        "                                                 n_mels=args.n_mels,\n",
        "                                                 power=2.0)\n",
        "\n",
        "mel = ToTensor()(librosa.power_to_db(\n",
        "    transform(waveform).squeeze().numpy(), ref=np.max))\n",
        "mel = mel.unsqueeze(0)\n",
        "scripted_module = torch.jit.load(model_path)\n",
        "pred = torch.argmax(scripted_module(mel), dim=1)\n",
        "print(f\"Ground Truth: {label}, Prediction: {idx_to_class[pred.item()]}\")"
      ],
      "metadata": {
        "id": "oYmuyAFDQ5gM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}